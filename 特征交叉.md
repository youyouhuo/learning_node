### 1.网络方式
* dcn   原理解读 ([知乎](https://zhuanlan.zhihu.com/p/120433070)) tensorflow实现[代码](https://github.com/tensorflow/recommenders/blob/main/tensorflow_recommenders/layers/feature_interaction/dcn.py)
  >> 论文
  >> * Deep & Cross Network for Ad Click Predictions
  >>>
  >>> * we stack the embedding vectors, along with the normalized dense features xdense, into one vector and feed x0 to the network.（离散型特征经过embedding后，和连续型特征拼接到一起作为最终的输入）
  >>>
  >>> * the small number of parameters of the cross network has limited the model capacity. To capture highly nonlinear interactions,
we introduce a deep network in parallel
  >>>
  >>> * 最后是deep层和cross层的的结果拼接到一起，乘以w后过激活函数 res = sigmoid(w*[xdeep,xcross])
  >>>
  >>> * Adam optimizer、 batch size is set at 512、Batch normalization was applied to the deep network 、 gradient clip norm
was set at 100、used early stopping， as we did not find L2 regularization or dropout to be effective
  >>>
  >>> * 这里的实现 tensorflow实现[代码](https://github.com/tensorflow/recommenders/blob/main/tensorflow_recommenders/layers/feature_interaction/dcn.py) 虽然是tensorflow的官方实现，但是和实际的论文有稍微差别,论文中的原始实现是 $x_{l+1} = x_{0} * x_{l} * w + b + x_{l}$ 而这里的实现是 $x_{l+1} = {x_0} * (w * x_{l} +b)+ x_{l}$有一些区别。
  >>>
 * DCN V2: Improved Deep & Cross Network and Practical Lessons
for Web-scale Learning to Rank Systems
>> 论文
  >> * DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems
  >>>
  >>> * dcn的缺点：
  >>>> * 1) The expressiveness of its cross network is limited (cross网络的表达能力有限)
  >>>> * 2) The polynomial class reproduced by the cross network is only characterized by 𝑂(input size) parameters, largely limiting its flexibility in modeling random cross patterns(cross网络再现的多项式类仅由 𝑂（输入大小）参数来表征，很大程度上限制了其对随机交叉模式建模的灵活性)
  >>>> * 3) the allocated capacity between the cross network and DNN is unbalanced，This gap significantly increases when applying DCN to large-scale production data. An overwhelming portion of the parameters will be used to learn implicit crosses in the DNN (cross网络和dnn网络分配的容量不平衡，当DCN应用于大规模生产数据时，这种差距显着增加。绝大多数参数将用于学习 DNN 中的隐式交叉)
>>  >>      
>>> * dcnv2 版本：
>>>>* 1) 采用是 $x_{l+1}=x_0*(w*x_{l}+b)+x_{l}$ *****计算
>>>>* 2) 输入部分也是离散型embedding后和连续型拼接到一起
>>>>* 3) 最后，在dcn和dnn的结合上，设计了类似dcn的并行方式  res=sigmoid(w*[xdeep,xcrossv2]) , 也有先dcnv2 后 dnn的串行方式
>>> * dcnv2的大改点：
>>>  >* Cost-Effective Mixture of Low-Rank DCN
>>>  >>* constrained by limited serving resources and strict latency requirements,we have to seek methods to reduce cost while
maintaining the accuracy（受到线上服务的限制，我们需要再保持模型准确性的情况下减少消耗)
>>  >>>* Low-rank techniques are widely used to reduce the computational cost. It approximates a dense matrix 𝑀 ∈ R𝑑×𝑑 by two tall and skinny matrices 𝑈 ,𝑉 ∈ R 𝑑×𝑟. When 𝑟 ≤ 𝑑/2, the cost will be reduced.(low-rank技术是将dxd的矩阵分解成2个dxr的矩阵，其中r<=d/2)
>>  >>>* 新增low-rank的U,V 来间接得到w 即 $x_{l+1}=x_0*(U_l*(V_l^T*x_{l})+b)+x_{l}$ 其中U和V都是dxr,而w是dxd，其中r<<d/2
>>  >>>> * 这里有2种解释：1）we learn feature crosses in a subspace(我们在子空间中学习特征交叉）2） we project the input x to lower-dimensional R𝑟,and then project it back to Rd（我们把输入x映射到低秩空间Rr，然后又映射会Rd)



