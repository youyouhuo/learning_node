### 1.ç½‘ç»œæ–¹å¼
* dcn   åŸç†è§£è¯» ([çŸ¥ä¹](https://zhuanlan.zhihu.com/p/120433070)) tensorflowå®ç°[ä»£ç ](https://github.com/tensorflow/recommenders/blob/main/tensorflow_recommenders/layers/feature_interaction/dcn.py)
  >> è®ºæ–‡
  >> * Deep & Cross Network for Ad Click Predictions
  >>>
  >>> * we stack the embedding vectors, along with the normalized dense features xdense, into one vector and feed x0 to the network.ï¼ˆç¦»æ•£å‹ç‰¹å¾ç»è¿‡embeddingåï¼Œå’Œè¿ç»­å‹ç‰¹å¾æ‹¼æ¥åˆ°ä¸€èµ·ä½œä¸ºæœ€ç»ˆçš„è¾“å…¥ï¼‰
  >>>
  >>> * the small number of parameters of the cross network has limited the model capacity. To capture highly nonlinear interactions,
we introduce a deep network in parallel
  >>>
  >>> * æœ€åæ˜¯deepå±‚å’Œcrosså±‚çš„çš„ç»“æœæ‹¼æ¥åˆ°ä¸€èµ·ï¼Œä¹˜ä»¥wåè¿‡æ¿€æ´»å‡½æ•° res = sigmoid(w*[xdeep,xcross])
  >>>
  >>> * Adam optimizerã€ batch size is set at 512ã€Batch normalization was applied to the deep network ã€ gradient clip norm
was set at 100ã€used early stoppingï¼Œ as we did not find L2 regularization or dropout to be effective
  >>>
  >>> * è¿™é‡Œçš„å®ç° tensorflowå®ç°[ä»£ç ](https://github.com/tensorflow/recommenders/blob/main/tensorflow_recommenders/layers/feature_interaction/dcn.py) è™½ç„¶æ˜¯tensorflowçš„å®˜æ–¹å®ç°ï¼Œä½†æ˜¯å’Œå®é™…çš„è®ºæ–‡æœ‰ç¨å¾®å·®åˆ«,è®ºæ–‡ä¸­çš„åŸå§‹å®ç°æ˜¯ $x_{l+1} = x_{0} * x_{l} * w + b + x_{l}$ è€Œè¿™é‡Œçš„å®ç°æ˜¯ $x_{l+1} = {x_0} * (w * x_{l} +b)+ x_{l}$æœ‰ä¸€äº›åŒºåˆ«ã€‚
  >>>
 * DCN V2: Improved Deep & Cross Network and Practical Lessons
for Web-scale Learning to Rank Systems
>> è®ºæ–‡
  >> * DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems
  >>>
  >>> * dcnçš„ç¼ºç‚¹ï¼š
  >>>> * 1) The expressiveness of its cross network is limited (crossç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›æœ‰é™)
  >>>> * 2) The polynomial class reproduced by the cross network is only characterized by ğ‘‚(input size) parameters, largely limiting its flexibility in modeling random cross patterns(crossç½‘ç»œå†ç°çš„å¤šé¡¹å¼ç±»ä»…ç”± ğ‘‚ï¼ˆè¾“å…¥å¤§å°ï¼‰å‚æ•°æ¥è¡¨å¾ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šé™åˆ¶äº†å…¶å¯¹éšæœºäº¤å‰æ¨¡å¼å»ºæ¨¡çš„çµæ´»æ€§)
  >>>> * 3) the allocated capacity between the cross network and DNN is unbalancedï¼ŒThis gap significantly increases when applying DCN to large-scale production data. An overwhelming portion of the parameters will be used to learn implicit crosses in the DNN (crossç½‘ç»œå’Œdnnç½‘ç»œåˆ†é…çš„å®¹é‡ä¸å¹³è¡¡ï¼Œå½“DCNåº”ç”¨äºå¤§è§„æ¨¡ç”Ÿäº§æ•°æ®æ—¶ï¼Œè¿™ç§å·®è·æ˜¾ç€å¢åŠ ã€‚ç»å¤§å¤šæ•°å‚æ•°å°†ç”¨äºå­¦ä¹  DNN ä¸­çš„éšå¼äº¤å‰)
>>  >>      
>>> * dcnv2 ç‰ˆæœ¬ï¼š
>>>>* 1) é‡‡ç”¨æ˜¯ $x_{l+1}=x_0*(w*x_{l}+b)+x_{l}$ *****è®¡ç®—
>>>>* 2) è¾“å…¥éƒ¨åˆ†ä¹Ÿæ˜¯ç¦»æ•£å‹embeddingåå’Œè¿ç»­å‹æ‹¼æ¥åˆ°ä¸€èµ·
>>>>* 3) æœ€åï¼Œåœ¨dcnå’Œdnnçš„ç»“åˆä¸Šï¼Œè®¾è®¡äº†ç±»ä¼¼dcnçš„å¹¶è¡Œæ–¹å¼  res=sigmoid(w*[xdeep,xcrossv2]) , ä¹Ÿæœ‰å…ˆdcnv2 å dnnçš„ä¸²è¡Œæ–¹å¼
>>> * dcnv2çš„å¤§æ”¹ç‚¹ï¼š
>>>  >* Cost-Effective Mixture of Low-Rank DCN
>>>  >>* constrained by limited serving resources and strict latency requirements,we have to seek methods to reduce cost while
maintaining the accuracyï¼ˆå—åˆ°çº¿ä¸ŠæœåŠ¡çš„é™åˆ¶ï¼Œæˆ‘ä»¬éœ€è¦å†ä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å‡å°‘æ¶ˆè€—)
>>  >>>* Low-rank techniques are widely used to reduce the computational cost. It approximates a dense matrix ğ‘€ âˆˆ Rğ‘‘Ã—ğ‘‘ by two tall and skinny matrices ğ‘ˆ ,ğ‘‰ âˆˆ R ğ‘‘Ã—ğ‘Ÿ. When ğ‘Ÿ â‰¤ ğ‘‘/2, the cost will be reduced.(low-rankæŠ€æœ¯æ˜¯å°†dxdçš„çŸ©é˜µåˆ†è§£æˆ2ä¸ªdxrçš„çŸ©é˜µï¼Œå…¶ä¸­r<=d/2)
>>  >>>* æ–°å¢low-rankçš„U,V æ¥é—´æ¥å¾—åˆ°w å³ $x_{l+1}=x_0*(U_l*(V_l^T*x_{l})+b)+x_{l}$ å…¶ä¸­Uå’ŒVéƒ½æ˜¯dxr,è€Œwæ˜¯dxdï¼Œå…¶ä¸­r<<d/2
>>  >>>> * è¿™é‡Œæœ‰2ç§è§£é‡Šï¼š1ï¼‰we learn feature crosses in a subspace(æˆ‘ä»¬åœ¨å­ç©ºé—´ä¸­å­¦ä¹ ç‰¹å¾äº¤å‰ï¼‰2ï¼‰ we project the input x to lower-dimensional Rğ‘Ÿ,and then project it back to Rdï¼ˆæˆ‘ä»¬æŠŠè¾“å…¥xæ˜ å°„åˆ°ä½ç§©ç©ºé—´Rrï¼Œç„¶ååˆæ˜ å°„ä¼šRd)



