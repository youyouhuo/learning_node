### 实时搜索策略调控
    状态s：点击的商品 + 用户长期行为特征
    奖励：最初版奖励：   pv下，1.如果点击，则为点击数量；2.如果购买，则为购买的价格；3.否则为0 
         升级版奖励： R = R(st,a,st+1) - 是否点击的交叉熵（这里是最后的得分） - 是否购买的交叉熵（这里是最后的得分）   （核心逻辑是，希望强化奖励，学习是否点击和是否购买）
    动作：输出 每个item的权重向量，最后是 排序的score = item_feature * action_theta


 升级版 ddpg，是 除了基础的ddpg学习，另外加上了 是否点击的交叉熵（这里是最后的得分） 和 是否购买的交叉熵（这里是最后的得分）的ltr loss。

### 组合优化视角下基于强化学 习的精准定向广告 OCPC 业务优化
排序的公式 $ctr^{\alpha} * bid * action$

动作为：调价比例

流量优化目标为: ${RPM+{\alpha} * GMV}$

理想情况下，学习 ${RPM+{\alpha} * GMV}$ 然后以此为目标进行降序排列，然后进行展现。但是这种方式下，违背了广告竞争中根据ecpm pk的商业逻辑，削弱了ocpc广告调价的意义，因此，目前希望达到的效果是，学习调价比例，然后使得 $ctr^{\alpha} * bid * action$ 的排序结果和 ${RPM+{\alpha} * GMV}$ 一样。


### 策略优化方法在搜索广告排序和竞价机制中的应用

目标：成不断提⾼ RPM （revenue per thousand impressions，即平均千次⼴告展⽰获得的总⼴告商扣费）兼顾 CTR（click-through-rate，⼴告点击率），CVR（conversion rate，转化率或者⽤户购买率）和 GMV （gross merchandise volume，电商货品总销量）等指标
实现目标的三个抓手：
    1.⽤户是否点击或者购买，只有点击平台才能获得收⼊，只有购买⼴告主的推⼴诉求才能被最直接地表达出来
    2.扣费，扣费的多少直接影响平台的收入，在gsp扣费计算公式下，相邻2个广告的排序分的紧凑程度会影响扣费，特别是个性化情况下。因此感知广告分布，预估扣费是一个手段。
    3.用户的浏览是一个过程，前一次的pv展示的响应会对后面的浏览产生影响，因此完整的优化要考虑整个浏览过程。
面向广告商，用户和平台收益的排序公式：
 $$g(s, a, ad) = f_{a_1} (ctr) * bid + a_2 * f_{a_3} (ctr, cvr) + a_4 * f_{a_5} (cvr, price)$$
 
 根据gsp计费，最终的收费可以表示为：
 $$click_{price} = \frac{g(s, a, ad^{second}) -  (a_2 * f_{a_3} (ctr, cvr) + a_4 * f_{a_5} (cvr, price))}{f_{a_1} (ctr) }$$
 
 这里ad和 $ad^{second}$分别表示相邻的第一个和第二个广告。
 
 系统设计：
  氛围离线模拟系统和线上系统，离线模拟充分探索可能组合。
  
  离线模拟时的奖励 $r = click_{price} * ctr + \delta * ctr$ 其中 $\delta$是调节因子，调整点击率和扣费之间的平衡。同时，这里预估的ctr和真是的ctr还是有区别的，所以这里需要进行标定（Calibration），采用标定后的数据计算奖励。
 
 强化学习模型的选择：考虑的2个因素 1）无法建模前后2个页面的关系，所以是一个offline-policy，2）动作是连续的 ，因此最终选择ddpg
 
 网络结构：    激活函数不使用sigmoid和relu，采用elu，同时critic采用 q（s，a）=v（s)+a（s,a)的方式。同时对动作进行clip。
  
