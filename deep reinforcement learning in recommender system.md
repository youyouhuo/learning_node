### å¼ºåŒ–å­¦ä¹ åœ¨æ¨èçš„åº”ç”¨

#0. Reinforcement Learning based Recommender Systems: A Survey ï¼ˆ2022 june acm)

#1. A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directionsï¼ˆarXiv-2021.09ï¼‰
  
 (blog)https://cloud.tencent.com/developer/article/1881235   ï¼ˆä¸Šé¢è®ºæ–‡å¯¹åº”çš„åšå®¢ï¼‰
 
 
#2. å¼ºåŒ–å­¦ä¹ åœ¨ç¾å›¢"çŒœä½ å–œæ¬¢"çš„åº”ç”¨ https://tech.meituan.com/2018/11/15/reinforcement-learning-in-mt-recommend-system.html

#3. å¼ºåŒ–å­¦ä¹ åœ¨äº¬ä¸œçš„åº”ç”¨ https://www.6aiq.com/article/1547826520120

#4. youtube value-baseçš„å¼ºåŒ–å­¦ä¹ æ¨èç³»ç»Ÿ https://blog.csdn.net/weixin_44289754/article/details/119122740

#5. åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨èç ”ç©¶ç»¼è¿° ï¼ˆè®¡ç®—æœºç§‘å­¦ï¼‰https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=20212

#6. å¼ºåŒ–å­¦ä¹ åœ¨ ç¾å›¢ äº¬ä¸œ å¾®è½¯çš„å®è·µï¼ˆçŸ¥ä¹ï¼‰ https://zhuanlan.zhihu.com/p/355041851

#7. (å¾®è½¯drn paperï¼‰DRN: A Deep Reinforcement Learning Framework for News Recommendation

#8. çŒœæ‚¨æ‰€æƒ³ï¼šæ·˜å®æœç´¢/æ¨èç³»ç»ŸèƒŒåæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸è‡ªé€‚åº”åœ¨çº¿å­¦ä¹ çš„å®è·µä¹‹è·¯ https://toutiao.io/posts/9j6ze2/preview ï¼ˆåŸå§‹é“¾æ¥ https://mp.weixin.qq.com/s/gKlyvv8hzlHRAOLRlVV3dw?ï¼‰

#9. ç»“åˆç”¨æˆ·é•¿çŸ­æœŸå…´è¶£çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨èæ–¹æ³•ï¼ˆä¸­æ–‡ä¿¡æ¯å­¦æŠ¥ï¼‰  http://jcip.cipsc.org.cn/CN/Y2021/V35/I8/107

#10. Flink + å¼ºåŒ–å­¦ä¹ æ­å»ºå®æ—¶æ¨èç³»ç»Ÿï¼ˆåšå®¢å›­---ç²¾è¯»ï¼‰  https://www.cnblogs.com/massquantity/p/13842139.html

#Top-ğ¾ Off-Policy Correction for a REINFORCE Recommender System
  æ•´ä½“ä¸Šï¼Œä½¿ç”¨äº†Chaos Free RNN (CFN) å»ºæ¨¡çŠ¶æ€ä¹‹é—´çš„è½¬ç§»
  ä½¿ç”¨ æ¢¯åº¦ä¸å›ä¼ çš„betaç½‘ç»œå¤„ç†ç¦»çº¿è®­ç»ƒä¸­çš„data biasï¼Œ
  ä¸ºäº†é™ä½çŠ¶æ€ç©ºé—´ï¼Œå…ˆä½¿ç”¨è¿‘é‚»æœç´¢Kä¸ªï¼Œç„¶åé¢„ä¼°è¿™kä¸ªactionçš„å€¼ã€‚
  ä¸ºäº†é™ä½æ–¹å·®ï¼Œä½¿ç”¨weight clipping
  ï¼ˆblog)http://wd1900.github.io/2019/06/23/Top-K-Off-Policy-Correction-for-a-REINFORCE-Recommender-System-on-Youtube/
  
 #slateQ ï¼ˆblog) https://blog.csdn.net/zackerzhuang/article/details/100978955   ä¸ https://blog.csdn.net/qq_16234613/article/details/105339645
    æ•´ä½“ä¸Šåœ¨æ’åºå±‚ä½¿ç”¨sarsaç®—æ³•æ¥è¿›è¡Œå­¦ä¹ ã€‚
    2ä¸ªåŸºæœ¬å‡è®¾ï¼š
              
                single choiceï¼šç”¨æˆ·ä¸€æ¬¡åªåœ¨æ¨èåˆ—è¡¨ä¸­ç‚¹å‡»ä¸€ä¸ªæ¨èé¡¹æˆ–ä¸ç‚¹å‡»ä»»ä½•æ¨èé¡¹
               
               RTDS(Reward/transition dependence on selection) ç”¨æˆ·æ ¹æ®é€‰æ‹©æ¨èé¡¹è€Œä¼šäº§ç”Ÿä¸åŒçš„å›æŠ¥å’ŒçŠ¶æ€è½¬ç§»ï¼Œæ²¡é€‰æ‹©æ—¶ï¼Œå°±æ˜¯æ²¡æœ‰è½¬ç§»ã€‚
    
 #jd-Deep Reinforcement Learning for List-wise Recommendations
 
 æ•´ç¯‡æ–‡ç« æ ¸å¿ƒæ˜¯å»ºç«‹ä¸€ä¸ªç¦»çº¿æ¨¡æ‹Ÿå™¨æ¥æ¨¡æ‹Ÿçº¿ä¸Šçš„æ¨èè¿‡ç¨‹ï¼Œæ²¡æœ‰è¿›è¡ŒçœŸå®çš„abæµ‹è¯•
 
 æ•´ä½“ä¸Šé‡‡ç”¨ddpgæ¥æ­å»ºï¼š
    
    æ¨¡æ‹Ÿæ—¶çš„å¥–åŠ±ï¼š
      ç”±äºæ˜¯ç¦»çº¿çš„æ¨¡æ‹Ÿï¼Œæ‰€ä»¥ä¸èƒ½ç›´æ¥æ”¶é›†åˆ°ç”¨æˆ·çš„åé¦ˆï¼Œè¿™é‡Œåšäº†ä¸€ä¸ªå‡è®¾ï¼šæœ‰ç›¸ä¼¼å…´è¶£çš„ç”¨æˆ·å¯¹åŒä¸€ä¸ªitemä¼šåšå‡ºç›¸ä¼¼çš„å†³ç­–ã€‚å› æ­¤åŸºäºæ­¤ï¼Œå¯ä»¥é€šè¿‡å†å²çš„<st,at> æ¥å¯¹æ¨¡æ‹Ÿæ—¶äº§ç”Ÿçš„çŠ¶æ€åŠ¨ä½œç”Ÿæˆå¥–åŠ±ã€‚
      ä½¿ç”¨ä¸€ä¸ªmemoryä¿å­˜ (s,a)->rçš„æ˜ å°„
      å¯¹äºç®—æ³•äº§ç”Ÿçš„(s,a)ï¼Œé€šè¿‡å’Œmemoryé‡Œé¢çš„ä¿æŒçš„æ˜ å°„è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç„¶åè¿‘ä¼¼äº§ç”Ÿæ˜ å°„ã€‚ï¼ˆè¿™é‡Œçš„sæ˜¯Nä¸ªæ­£åé¦ˆçš„itemæ„æˆçš„ï¼Œæ¯æ¬¡æœ‰æ–°çš„itemï¼Œå°±åˆ é™¤æœ€æ—§çš„é‚£ä¸ªitemï¼‰
    
    åŠ¨ä½œç½‘ç»œï¼š 
        æ¯æ¬¡æ¨èkä¸ªitem
        s->wt   é€šè¿‡çŠ¶æ€å­¦ä¹ kä¸ªw
        åŠ¨ä½œäº§ç”Ÿï¼š scoret=wt*embi    é€šè¿‡è¿™kä¸ªwä¹˜ä»¥itemçš„embeddingæ¥è®¡ç®—æœ€ç»ˆçš„å¾—åˆ†ï¼Œç„¶åæ¯ä¸ªä½ç½®å–å¾—åˆ†æœ€é«˜çš„itemä½œä¸ºæœ€ç»ˆåŠ¨ä½œaiï¼ˆå‰é¢ä½ç½®å–äº†å°±ä»åé¢ä½ç½®çš„å€™é€‰ä¸­åˆ é™¤ï¼‰
    criticç½‘ç»œï¼š
        è¾“å…¥st å’Œ ai......ak ï¼Œè¾“å‡ºQ(st,at)    è¿™é‡Œatæ˜¯ kä¸ªitemçš„ç»Ÿ-è¡¨ç¤º
    
    æ•´ä½“æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š    
        
        æ ¹æ®ç”¨æˆ·çš„sessionæ»‘åŠ¨ï¼Œéå†æ¯ä¸€ä¸ªsession,
        æ ¹æ®å‰ä¸€ä¸ªsessionå¾—åˆ°çš„çŠ¶æ€sä½œä¸ºs0ï¼Œè¿›è¡Œé¢„æµ‹çŠ¶æ€atï¼Œå¹¶è®¡ç®—å¯¹åº”çš„å¥–åŠ±
        æ›´æ–°çŠ¶æ€ï¼Œå¹¶æŠŠstï¼Œatï¼Œrtï¼Œst+1ä¿å­˜èµ·æ¥ï¼Œä¿å­˜ä¸ºD
        
        ä»Dä¸­é‡‡æ ·Nä¸ªæ ·æœ¬ï¼Œæ›´æ–°acotrå’Œcriticçš„çš„å‚æ•°
    
