### 链接见：https://llmbook-zh.github.io/


### 笔记
> 1.各代模型的功能情况

      - 1990s: n-gram统计模型【具备一定生成能力，辅助解决部分任务，数据系数影响严重】，

      - 2013【RNN-LM，word2vec，有效克服数据系数问题，无监督学习语义特征表示，缺乏知识、可迁移性差】，
    
      - 2018【ELMo,BERT,GPT-1/2，有效捕捉上下文语义，任务迁移性有了显著提升，仍然需要监督数据微调】，
    
      - 2022【GPT3/4,ChatGPT,Claude，规模扩展带来性能重要提升，通用的任务求解途径，学习成本高，适配灵活性差】

>2. 大语言模型的能力特点

      - 具有丰富的世界知识【参数规模大，数据规模大，充分学习到海量世界知识】

      - 具有较强的通用任务解决能力 【下一个词预测任务可以看成是一个多任务学习过程，不同词的预测可能涉及到不同训练任务，如请个分类 xxx真好看，数值计算，xxx=7，知识推理 中国最大的省份是新疆】

      - 具有较好的复杂任务推理能力

      - 具有较强的人类指令遵循能力【任务输入与执行结果均通过自然语言进行表达。通过预训练和微调，大模型具备人类指令遵循能力】

      - 具有较好的人类对齐能力。【通过强化学习使得模型进行正确行为的加强以及错误行为的规避，进而建立较好的人类对齐能力】

      - 具有可拓展的工具使用能力【可以通过微调、上下文学习等方式掌握外部工具的使用、如搜索引擎和计算器】

> 3. 大语言模型关键技术概览

      — 规模扩展 【参数、数据、算力】
      
      — 数据工程 【数据全面的采集，拓宽高质量的数据来源；对数据进行精细的清晰，提升数据质量；设计有效的数据配比与数据课程，加强模型对数据语义信息的利用效率】

      - 高效的预训练 【大规模分布式训练算法优化参数。联合并行策略以及效率优化方法，包括3D并行【数据并行，流水线并行、张量并行】，ZeRO【内存冗余消除技术】等；专用的分布式优化框架建华并行算法的实现和部署【如deepSpeed和megatron-LM等】；全栈式的优化体系架构，【支持大规模预训练数据的调度安排，建立起可迭代的模型性能改进闭环，加强效果反馈机制，从而能够快速、灵活地进行相关训练策略的调整】
