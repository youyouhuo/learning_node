# policy search read note

truely random search:

    (r+ - r-) ä¼°è®¡æ¢¯åº¦çš„è§£é‡Šï¼ˆå¯¹ç§°é‡‡æ ·æ³•ï¼‰ ï¼ˆPGPE) https://paperexplained.cn/articles/article/sdetail/a6607282-20d0-4bbc-a45f-a7c3f122c206/

population-based:
  Population-based BBO methods manage a limited population of individuals, and generate new individuals randomly in the vicinity of the previous elite individuals.
  
Evolutionary strategies:
  Evolutionary strategies (ES) can be seen as specific population-based optimization methods where only one individual is retained from one generation to the next. More specifically, an optimum guess is computed from the previous samples, then the next samples are obtained by adding Gaussian noise to the current optimum guess.
    
    åšå®¢ï¼švisual evolutionary strategies(ç¿»è¯‘ç‰ˆï¼‰ https://www.yanxishe.com/blogDetail/5791

ç­–ç•¥æ¢¯åº¦çš„ç®€ä»‹ï¼š
   
   https://lilianweng.github.io/posts/2018-04-08-policy-gradient/


( ppt) Natural Policy Gradients, TRPO, PPO
     
     https://www.andrew.cmu.edu/course/10-703/slides/Lecture_NaturalPolicyGradientsTRPOPPO.pdf
 
(ppt)  Augmented Random Search
    
     https://cse.buffalo.edu/~avereshc/rl_spring20/Augmented_Random_Search_Gautam_Suryawanshi_Prajit_Krisshna_Kumar.pdf

ï¼ˆblog) augmented Random Search
    
    https://towardsdatascience.com/introduction-to-augmented-random-search-d8d7b55309bd
   
(cem) Cross-Entropy for Monte-Carlo Tree Search

 (blog) natual policy gradients 
    
    https://jonathan-hui.medium.com/rl-natural-policy-gradient-actor-critic-using-kronecker-factored-trust-region-acktr-58f3798a4a93

ï¼ˆblog) ddpg
    
    https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b

ï¼ˆppt)ddpg
 
    http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring18/ujjawal/DDPG-Algorithm.pdf
    
 ï¼ˆblog) Bayesian Optimization
 
    https://machinelearningmastery.com/what-is-bayesian-optimization/
  
  (paper) BOA The Bayesian Optimization Algorithm
  
    https://dl.acm.org/doi/pdf/10.5555/2933923.2933973
  
  Bayesian Optimization çš„ acquisition functionï¼š 
    
    https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html
 
 (blog) bayesian optimization hyperparameter
 
 https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f
 
 (paper) A tutorial on
bayesian optimization of expensive cost functions, with
application to active user modeling and hierarchical reinforcement
learning.

    https://www.cs.ubc.ca/~nando/papers/bayopt.pdf
    
  ï¼ˆblogï¼‰ open-es æ¢¯åº¦æ¨å¯¼
  
  https://stats.stackexchange.com/questions/348111/how-is-the-equation-in-evolution-strategies-as-a-scalable-alternative-to-reinfo/348221#348221
  
  (blog) es 
  
  https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/
  
  (blog) epsion-greedy vs softmax  q-leanring vs sarsa
  
  https://ai.stackexchange.com/questions/17603/what-is-the-difference-between-the-epsilon-greedy-and-softmax-policies
    
    The ğœ–-greedy policy is a policy that chooses the best action 
    (i.e. the action associated with the highest value) with probability 1âˆ’ğœ–âˆˆ[0,1] and a random action with probability ğœ–. 
    The problem with ğœ–-greedy is that, when it chooses the random actions (i.e. with probability ğœ–), 
    it chooses them uniformly (i.e. it considers all actions equally good), 
    even though certain actions (even excluding the currently best one) are better than others. 
    Of course, this approach is not ideal in the case certain actions are extremely worse than others. 
    Therefore, a natural solution to this problem is to select the random actions with probabilities proportional to their current values. 
    These policies are called softmax policies.
  
  ï¼ˆblog) mdp
  
  https://juejin.cn/post/6859330833879154696
    
    å¼ºåŒ–å­¦ä¹ ä»»åŠ¡é€šå¸¸ä½¿ç”¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Processï¼Œç®€ç§°MDPï¼‰æ¥æè¿°ï¼Œå…·ä½“è€Œè¨€ï¼š
        æœºå™¨å¤„åœ¨ä¸€ä¸ªç¯å¢ƒä¸­ï¼Œæ¯ä¸ªçŠ¶æ€ä¸ºæœºå™¨å¯¹å½“å‰ç¯å¢ƒçš„æ„ŸçŸ¥ï¼›
        æœºå™¨åªèƒ½é€šè¿‡åŠ¨ä½œæ¥å½±å“ç¯å¢ƒï¼Œå½“æœºå™¨æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œåï¼Œä¼šä½¿å¾—ç¯å¢ƒæŒ‰æŸç§æ¦‚ç‡è½¬ç§»åˆ°å¦ä¸€ä¸ªçŠ¶æ€ï¼›
        åŒæ—¶ï¼Œç¯å¢ƒä¼šæ ¹æ®æ½œåœ¨çš„å¥–èµå‡½æ•°åé¦ˆç»™æœºå™¨ä¸€ä¸ªå¥–èµã€‚
        ç»¼åˆè€Œè¨€ï¼Œå¼ºåŒ–å­¦ä¹ ä¸»è¦åŒ…å«å››ä¸ªè¦ç´ ï¼šçŠ¶æ€ã€åŠ¨ä½œã€è½¬ç§»æ¦‚ç‡ä»¥åŠå¥–èµå‡½æ•°ã€‚
        
 
 (blog) actor-critic 
    
 https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f
    
 (blog) multi-armed-bandit
    
 https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/
 
 (paper) cem
 
 CROSS-ENTROPY FOR MONTE-CARLO TREE SEARCH
 Learning Tetris Using the Noisy Cross-Entropy Method
 A Tutorial on the Cross-Entropy Method Pieter-Tjerk de Boer
    
